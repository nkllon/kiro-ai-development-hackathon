#!/usr/bin/env python3
"""
Neo4j Round-Trip Test Suite

This script tests the complete round-trip:
1. Model â†’ Cypher â†’ Database
2. Database â†’ Query â†’ Results
3. Results â†’ Model validation

Critical for ensuring our model-driven approach works end-to-end.
"""

import json
import os
import time
from typing import Any

from neo4j import GraphDatabase


class Neo4jRoundTripTester:
    """Comprehensive round-trip testing for Neo4j integration"""

    def __init__(
        self,
        uri: str = "neo4j://localhost:7687",
        username: str = os.getenv("NEO4J_USERNAME", "neo4j"),
        password: str = os.getenv("NEO4J_PASSWORD", ""),
    ):
        self.driver = GraphDatabase.driver(uri, auth=(username, password))
        self.test_results = []

    def close(self):
        """Close the database connection"""
        self.driver.close()

    def clear_database(self) -> None:
        """Clear all data from the database before testing"""
        print("ğŸ§¹ Clearing database before testing...")

        try:
            with self.driver.session() as session:
                # Delete all relationships first
                session.run("MATCH ()-[r]-() DELETE r")
                # Delete all nodes
                session.run("MATCH (n) DELETE n")

                # Verify database is empty
                node_count = session.run("MATCH (n) RETURN count(n) as count").single()["count"]
                rel_count = session.run("MATCH ()-[r]-() RETURN count(r) as count").single()["count"]

                if node_count == 0 and rel_count == 0:
                    print("    âœ… Database cleared successfully")
                else:
                    print(f"    âš ï¸  Database not fully cleared: {node_count} nodes, {rel_count} relationships remain")

        except Exception as e:
            print(f"    âŒ Failed to clear database: {e}")
            raise

    def run_round_trip_tests(self) -> dict[str, Any]:
        """Run all round-trip tests"""
        print("ğŸš€ Starting Neo4j Round-Trip Tests...")
        print("=" * 60)

        try:
            # Clear database before testing
            self.clear_database()

            # Test 1: Model to Database Population
            self.test_model_to_database()

            # Test 2: Database Query Validation
            self.test_database_queries()

            # Test 3: Results to Model Validation
            self.test_results_to_model()

            # Test 4: Edge Cases and Error Handling
            self.test_edge_cases()

            # Test 5: Performance and Scalability
            self.test_performance()

            # Generate comprehensive report
            return self.generate_test_report()

        except Exception as e:
            print(f"âŒ Round-trip tests failed: {e}")
            raise
        finally:
            self.close()

    def test_model_to_database(self) -> None:
        """Test 1: Populate database from model"""
        print("ğŸ”§ Test 1: Model â†’ Database Population")

        try:
            # Load the project model using Model Registry tools
            from src.round_trip_engineering.tools import get_model_registry

            registry = get_model_registry()
            manager = registry.get_model("project")
            model = manager.load_model()

            # Generate Cypher queries
            from scripts.neo4j_poc import Neo4jPOC

            poc = Neo4jPOC()
            poc.load_project_model()
            queries = poc.generate_cypher_queries()

            # Execute queries to populate database
            with self.driver.session() as session:
                for i, query in enumerate(queries):
                    try:
                        result = session.run(query)
                        # Consume result to ensure execution
                        list(result)
                        print(f"    âœ… Query {i + 1}: {query[:50]}...")
                    except Exception as e:
                        print(f"    âŒ Query {i + 1} failed: {e}")
                        raise

            # Verify data was created
            with self.driver.session() as session:
                # Count nodes
                domain_count = session.run("MATCH (d:Domain) RETURN count(d) as count").single()["count"]
                rule_count = session.run("MATCH (r:Rule) RETURN count(r) as count").single()["count"]
                relationship_count = session.run("MATCH ()-[r:CONTAINS]->() RETURN count(r) as count").single()["count"]

                print(f"    ğŸ“Š Created {domain_count} domains, {rule_count} rules, {relationship_count} relationships")

                # Validate against model
                expected_rules = len(model["domain_architecture"]["cursor_rules"]["emoji_prefixes"])
                if rule_count == expected_rules:
                    print(f"    âœ… Rule count matches model: {rule_count} == {expected_rules}")
                else:
                    print(f"    âŒ Rule count mismatch: {rule_count} != {expected_rules}")

            self.test_results.append(
                {
                    "test": "model_to_database",
                    "status": "passed",
                    "details": f"Created {domain_count} domains, {rule_count} rules, {relationship_count} relationships",
                }
            )

        except Exception as e:
            print(f"    âŒ Model to database test failed: {e}")
            self.test_results.append({"test": "model_to_database", "status": "failed", "error": str(e)})

    def test_database_queries(self) -> None:
        """Test 2: Validate database queries return expected results"""
        print("\nğŸ” Test 2: Database Query Validation")

        test_queries = [
            {
                "name": "Domain Count",
                "query": "MATCH (d:Domain) RETURN count(d) as count",
                "expected": {"count": 1},
            },
            {
                "name": "Rule Count",
                "query": "MATCH (r:Rule) RETURN count(r) as count",
                "expected": {"count": 21},
            },
            {
                "name": "Relationship Count",
                "query": "MATCH ()-[r:CONTAINS]->() RETURN count(r) as count",
                "expected": {"count": 21},
            },
            {
                "name": "Cursor Rules Domain",
                "query": "MATCH (d:Domain {name: 'cursor_rules'}) RETURN d.name, d.status",
                "expected": {"d.name": "cursor_rules", "d.status": "completed"},
            },
            {
                "name": "Security Rule",
                "query": "MATCH (r:Rule {name: 'security'}) RETURN r.emoji, r.type",
                "expected": {"r.emoji": "ğŸ”’", "r.type": "cursor_rule"},
            },
        ]

        passed = 0
        total = len(test_queries)

        with self.driver.session() as session:
            for test in test_queries:
                try:
                    result = session.run(test["query"]).single()
                    if result:
                        # Convert Record to dict for comparison
                        result_dict = dict(result)
                        # Check if all expected values match
                        matches = all(result_dict.get(key) == value for key, value in test["expected"].items())
                        if matches:
                            print(f"    âœ… {test['name']}: {result_dict}")
                            passed += 1
                        else:
                            print(f"    âŒ {test['name']}: Expected {test['expected']}, got {result_dict}")
                    else:
                        print(f"    âŒ {test['name']}: No results returned")

                except Exception as e:
                    print(f"    âŒ {test['name']} failed: {e}")

        print(f"    ğŸ“Š Query validation: {passed}/{total} passed")

        self.test_results.append(
            {
                "test": "database_queries",
                "status": "passed" if passed == total else "failed",
                "details": f"{passed}/{total} queries passed",
            }
        )

    def test_results_to_model(self) -> None:
        """Test 3: Validate database results against model"""
        print("\nğŸ”„ Test 3: Results â†’ Model Validation")

        try:
            # Load model using Model Registry tools
            from src.round_trip_engineering.tools import get_model_registry

            registry = get_model_registry()
            manager = registry.get_model("project")
            model = manager.load_model()

            expected_rules = model["domain_architecture"]["cursor_rules"]["emoji_prefixes"]

            # Query database for all rules
            with self.driver.session() as session:
                result = session.run("MATCH (r:Rule) RETURN r.name, r.emoji ORDER BY r.name")
                db_rules = {row["r.name"]: row["r.emoji"] for row in result}

            # Validate against model
            validation_errors = []
            for rule_name, expected_emoji in expected_rules.items():
                if rule_name not in db_rules:
                    validation_errors.append(f"Rule '{rule_name}' missing from database")
                elif db_rules[rule_name] != expected_emoji:
                    validation_errors.append(f"Rule '{rule_name}' emoji mismatch: expected {expected_emoji}, got {db_rules[rule_name]}")

            if not validation_errors:
                print(f"    âœ… All {len(expected_rules)} rules validated against model")
                print(f"    ğŸ“Š Database rules: {list(db_rules.keys())}")

                self.test_results.append(
                    {
                        "test": "results_to_model",
                        "status": "passed",
                        "details": f"All {len(expected_rules)} rules validated",
                    }
                )
            else:
                print(f"    âŒ Validation errors found:")
                for error in validation_errors:
                    print(f"        - {error}")

                self.test_results.append(
                    {
                        "test": "results_to_model",
                        "status": "failed",
                        "errors": validation_errors,
                    }
                )

        except Exception as e:
            print(f"    âŒ Results to model validation failed: {e}")
            self.test_results.append({"test": "results_to_model", "status": "failed", "error": str(e)})

    def test_edge_cases(self) -> None:
        """Test 4: Edge cases and error handling"""
        print("\nâš ï¸  Test 4: Edge Cases and Error Handling")

        edge_case_tests = [
            {
                "name": "Invalid Cypher Syntax",
                "query": "CREATE (invalid syntax",
                "should_fail": True,
            },
            {
                "name": "Non-existent Node",
                "query": "MATCH (n:NonExistent) RETURN n",
                "should_fail": False,  # Should return empty result
                "expected_count": 0,
            },
            {
                "name": "Empty Result Set",
                "query": "MATCH (r:Rule {name: 'non_existent_rule'}) RETURN r",
                "should_fail": False,
                "expected_count": 0,
            },
            {
                "name": "Complex Query",
                "query": "MATCH (d:Domain)-[:CONTAINS]->(r:Rule) WHERE r.emoji CONTAINS 'ğŸ”’' RETURN d.name, r.name, r.emoji ORDER BY r.name",
                "should_fail": False,
            },
        ]

        passed = 0
        total = len(edge_case_tests)

        with self.driver.session() as session:
            for test in edge_case_tests:
                try:
                    if test["should_fail"]:
                        # Expect this to fail
                        try:
                            result = session.run(test["query"])
                            list(result)  # Consume result
                            print(f"    âŒ {test['name']}: Expected to fail but succeeded")
                        except Exception:
                            print(f"    âœ… {test['name']}: Correctly failed as expected")
                            passed += 1
                    else:
                        # Expect this to succeed
                        result = session.run(test["query"])
                        results = list(result)

                        if "expected_count" in test:
                            if len(results) == test["expected_count"]:
                                print(f"    âœ… {test['name']}: Correct count {len(results)}")
                                passed += 1
                            else:
                                print(f"    âŒ {test['name']}: Expected {test['expected_count']}, got {len(results)}")
                        else:
                            print(f"    âœ… {test['name']}: Executed successfully, returned {len(results)} results")
                            passed += 1

                except Exception as e:
                    if test["should_fail"]:
                        print(f"    âœ… {test['name']}: Correctly failed as expected")
                        passed += 1
                    else:
                        print(f"    âŒ {test['name']}: Unexpected failure: {e}")

        print(f"    ğŸ“Š Edge case tests: {passed}/{total} passed")

        self.test_results.append(
            {
                "test": "edge_cases",
                "status": "passed" if passed == total else "failed",
                "details": f"{passed}/{total} edge case tests passed",
            }
        )

    def test_performance(self) -> None:
        """Test 5: Performance and scalability"""
        print("\nâš¡ Test 5: Performance and Scalability")

        try:
            with self.driver.session() as session:
                # Test query execution time
                start_time = time.time()
                result = session.run("MATCH (d:Domain)-[:CONTAINS]->(r:Rule) RETURN d.name, r.name, r.emoji ORDER BY r.name")
                results = list(result)
                execution_time = time.time() - start_time

                print(f"    â±ï¸  Complex query execution time: {execution_time:.4f}s")
                print(f"    ğŸ“Š Results returned: {len(results)}")

                # Performance thresholds
                if execution_time < 0.1:  # 100ms threshold
                    print(f"    âœ… Performance: Excellent (< 100ms)")
                    performance_status = "excellent"
                elif execution_time < 0.5:  # 500ms threshold
                    print(f"    âœ… Performance: Good (< 500ms)")
                    performance_status = "good"
                else:
                    print(f"    âš ï¸  Performance: Slow ({execution_time:.4f}s)")
                    performance_status = "slow"

                # Test memory usage (approximate)
                memory_estimate = len(results) * 100  # Rough estimate: 100 bytes per result
                print(f"    ğŸ’¾ Memory usage estimate: ~{memory_estimate} bytes")

                self.test_results.append(
                    {
                        "test": "performance",
                        "status": "passed",
                        "details": f"Execution time: {execution_time:.4f}s, Performance: {performance_status}",
                    }
                )

        except Exception as e:
            print(f"    âŒ Performance test failed: {e}")
            self.test_results.append({"test": "performance", "status": "failed", "error": str(e)})

    def generate_test_report(self) -> dict[str, Any]:
        """Generate comprehensive test report"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ROUND-TRIP TEST REPORT")
        print("=" * 60)

        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result["status"] == "passed")
        failed_tests = total_tests - passed_tests

        print(f"ğŸ¯ Total Tests: {total_tests}")
        print(f"âœ… Passed: {passed_tests}")
        print(f"âŒ Failed: {failed_tests}")
        print(f"ğŸ“ˆ Success Rate: {(passed_tests / total_tests) * 100:.1f}%")

        # Detailed results
        print("\nğŸ“‹ Detailed Results:")
        for result in self.test_results:
            status_emoji = "âœ…" if result["status"] == "passed" else "âŒ"
            print(f"  {status_emoji} {result['test']}: {result['status']}")
            if "details" in result:
                print(f"      Details: {result['details']}")
            if "error" in result:
                print(f"      Error: {result['error']}")
            if "errors" in result:
                for error in result["errors"]:
                    print(f"      - {error}")

        # Overall assessment
        if failed_tests == 0:
            print(f"\nğŸ‰ ALL TESTS PASSED! Round-trip validation successful!")
            overall_status = "SUCCESS"
        elif passed_tests > failed_tests:
            print(f"\nâš ï¸  Most tests passed, but {failed_tests} failed. Review needed.")
            overall_status = "PARTIAL_SUCCESS"
        else:
            print(f"\nâŒ {failed_tests} tests failed. Major issues detected.")
            overall_status = "FAILURE"

        report = {
            "overall_status": overall_status,
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "success_rate": (passed_tests / total_tests) * 100,
            "test_results": self.test_results,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        }

        # Save report to file
        with open("neo4j_round_trip_report.json", "w") as f:
            json.dump(report, f, indent=2)

        print(f"\nğŸ“„ Detailed report saved to: neo4j_round_trip_report.json")

        return report


def main():
    """Main execution function"""
    tester = Neo4jRoundTripTester()

    try:
        report = tester.run_round_trip_tests()

        # Exit with appropriate code
        if report["overall_status"] == "SUCCESS":
            print("\nğŸš€ Round-trip tests completed successfully!")
            exit(0)
        else:
            print(f"\nâš ï¸  Round-trip tests completed with issues: {report['overall_status']}")
            exit(1)

    except Exception as e:
        print(f"\nâŒ Round-trip tests failed: {e}")
        exit(1)


if __name__ == "__main__":
    main()
