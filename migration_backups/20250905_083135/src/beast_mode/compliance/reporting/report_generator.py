"""
Comprehensive compliance report generation system.

This module provides the ReportGenerator class that creates detailed compliance
reports with severity categorization, issue details, and actionable remediation
guidance for the Beast Mode Framework compliance checking system.
"""

from typing import List, Dict, Any, Optional
from datetime import datetime
from dataclasses import dataclass
import json

from ..interfaces import ComplianceReporter
from ..models import (
    ComplianceAnalysisResult,
    ComplianceIssue,
    IssueSeverity,
    ComplianceIssueType,
    RemediationStep,
    Phase2ValidationResult
)


@dataclass
class ComplianceReport:
    """Comprehensive compliance report with all analysis results."""
    report_id: str
    generation_timestamp: datetime
    analysis_result: ComplianceAnalysisResult
    executive_summary: str
    detailed_findings: Dict[str, Any]
    remediation_plan: List[RemediationStep]
    phase3_readiness_assessment: Dict[str, Any]
    formatted_report: str


@dataclass
class ComplianceSummary:
    """High-level compliance summary for quick assessment."""
    overall_score: float
    total_issues: int
    critical_issues: int
    high_priority_issues: int
    phase3_ready: bool
    key_blockers: List[str]
    next_actions: List[str]


class ReportGenerator(ComplianceReporter):
    """
    Comprehensive compliance report generator.
    
    Generates detailed compliance reports with severity categorization,
    issue details, and actionable remediation guidance.
    """
    
    def __init__(self):
        """Initialize the report generator."""
        self.report_format = "markdown"
        self.severity_weights = {
            IssueSeverity.CRITICAL: 4.0,
            IssueSeverity.HIGH: 3.0,
            IssueSeverity.MEDIUM: 2.0,
            IssueSeverity.LOW: 1.0
        }
    
    def generate_report(self, analysis_result: ComplianceAnalysisResult) -> str:
        """
        Generate a comprehensive compliance report.
        
        Args:
            analysis_result: The compliance analysis results
            
        Returns:
            Formatted compliance report as markdown string
        """
        report_id = f"compliance-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        
        # Generate all report components
        executive_summary = self._generate_executive_summary(analysis_result)
        detailed_findings = self._generate_detailed_findings(analysis_result)
        remediation_plan = self._generate_remediation_plan(analysis_result)
        phase3_assessment = self._generate_phase3_readiness_assessment(analysis_result)
        
        # Format the complete report
        formatted_report = self._format_complete_report(
            report_id, analysis_result, executive_summary, 
            detailed_findings, remediation_plan, phase3_assessment
        )
        
        return formatted_report
    
    def generate_compliance_summary(self, analysis_result: ComplianceAnalysisResult) -> ComplianceSummary:
        """
        Generate a high-level compliance summary.
        
        Args:
            analysis_result: The compliance analysis results
            
        Returns:
            ComplianceSummary with key metrics and status
        """
        all_issues = self._collect_all_issues(analysis_result)
        
        critical_issues = [i for i in all_issues if i.severity == IssueSeverity.CRITICAL]
        high_issues = [i for i in all_issues if i.severity == IssueSeverity.HIGH]
        
        key_blockers = [issue.description for issue in critical_issues[:5]]  # Top 5 blockers
        next_actions = self._generate_next_actions(analysis_result)
        
        return ComplianceSummary(
            overall_score=analysis_result.overall_compliance_score,
            total_issues=len(all_issues),
            critical_issues=len(critical_issues),
            high_priority_issues=len(high_issues),
            phase3_ready=analysis_result.phase3_ready,
            key_blockers=key_blockers,
            next_actions=next_actions
        )
    
    def get_report_format(self) -> str:
        """Get the format of reports generated by this reporter."""
        return self.report_format
    
    def _generate_executive_summary(self, analysis_result: ComplianceAnalysisResult) -> str:
        """Generate executive summary of compliance analysis."""
        all_issues = self._collect_all_issues(analysis_result)
        critical_count = len([i for i in all_issues if i.severity == IssueSeverity.CRITICAL])
        high_count = len([i for i in all_issues if i.severity == IssueSeverity.HIGH])
        
        status = "READY" if analysis_result.phase3_ready else "NOT READY"
        
        summary = f"""
## Executive Summary

**Overall Compliance Score:** {analysis_result.overall_compliance_score:.1f}/100.0

**Phase 3 Readiness:** {status}

**Key Metrics:**
- Total Issues Found: {len(all_issues)}
- Critical Issues: {critical_count}
- High Priority Issues: {high_count}
- Test Coverage: {analysis_result.test_coverage_status.current_coverage:.1f}% (Baseline: {analysis_result.test_coverage_status.baseline_coverage:.1f}%)
- RDI Compliance Score: {analysis_result.rdi_compliance.compliance_score:.1f}/100.0
- RM Compliance Score: {analysis_result.rm_compliance.compliance_score:.1f}/100.0

**Analysis Scope:**
- Commits Analyzed: {len(analysis_result.commits_analyzed)}
- Files Changed: {sum(len(commit.modified_files) + len(commit.added_files) for commit in analysis_result.commits_analyzed)}
- Analysis Timestamp: {analysis_result.analysis_timestamp.strftime('%Y-%m-%d %H:%M:%S')}
        """.strip()
        
        return summary
    
    def _generate_detailed_findings(self, analysis_result: ComplianceAnalysisResult) -> Dict[str, Any]:
        """Generate detailed findings by category."""
        findings = {
            "rdi_compliance": self._analyze_rdi_findings(analysis_result.rdi_compliance),
            "rm_compliance": self._analyze_rm_findings(analysis_result.rm_compliance),
            "test_coverage": self._analyze_test_coverage_findings(analysis_result.test_coverage_status),
            "task_reconciliation": self._analyze_task_reconciliation_findings(analysis_result.task_completion_reconciliation),
            "commit_analysis": self._analyze_commit_findings(analysis_result.commits_analyzed)
        }
        
        return findings
    
    def _generate_remediation_plan(self, analysis_result: ComplianceAnalysisResult) -> List[RemediationStep]:
        """Generate comprehensive remediation plan."""
        all_issues = self._collect_all_issues(analysis_result)
        remediation_steps = []
        
        # Group issues by type and severity
        issue_groups = self._group_issues_by_type_and_severity(all_issues)
        
        step_counter = 1
        for issue_type, severity_groups in issue_groups.items():
            for severity, issues in severity_groups.items():
                if not issues:
                    continue
                
                step = RemediationStep(
                    step_id=f"STEP-{step_counter:03d}",
                    description=self._generate_remediation_description(issue_type, severity, issues),
                    priority=severity,
                    estimated_effort=self._estimate_remediation_effort(issues),
                    affected_components=self._extract_affected_components(issues),
                    prerequisites=self._determine_prerequisites(issue_type, severity),
                    validation_criteria=self._generate_validation_criteria(issue_type, issues)
                )
                remediation_steps.append(step)
                step_counter += 1
        
        # Sort by priority (critical first)
        remediation_steps.sort(key=lambda x: self.severity_weights[x.priority], reverse=True)
        
        return remediation_steps
    
    def _generate_phase3_readiness_assessment(self, analysis_result: ComplianceAnalysisResult) -> Dict[str, Any]:
        """Generate Phase 3 readiness assessment."""
        all_issues = self._collect_all_issues(analysis_result)
        blocking_issues = [i for i in all_issues if i.blocking_merge]
        
        readiness_factors = {
            "rdi_compliance": {
                "score": analysis_result.rdi_compliance.compliance_score,
                "status": "PASS" if analysis_result.rdi_compliance.compliance_score >= 80.0 else "FAIL",
                "requirements_traced": analysis_result.rdi_compliance.requirements_traced,
                "design_aligned": analysis_result.rdi_compliance.design_aligned,
                "implementation_complete": analysis_result.rdi_compliance.implementation_complete
            },
            "rm_compliance": {
                "score": analysis_result.rm_compliance.compliance_score,
                "status": "PASS" if analysis_result.rm_compliance.compliance_score >= 80.0 else "FAIL",
                "interface_implemented": analysis_result.rm_compliance.interface_implemented,
                "size_constraints_met": analysis_result.rm_compliance.size_constraints_met,
                "health_monitoring_present": analysis_result.rm_compliance.health_monitoring_present
            },
            "test_coverage": {
                "current_coverage": analysis_result.test_coverage_status.current_coverage,
                "baseline_coverage": analysis_result.test_coverage_status.baseline_coverage,
                "status": "PASS" if analysis_result.test_coverage_status.coverage_adequate else "FAIL",
                "failing_tests_count": len(analysis_result.test_coverage_status.failing_tests)
            },
            "blocking_issues": {
                "count": len(blocking_issues),
                "critical_blockers": [i.description for i in blocking_issues if i.severity == IssueSeverity.CRITICAL],
                "status": "PASS" if len(blocking_issues) == 0 else "FAIL"
            }
        }
        
        # Calculate overall readiness score
        factor_scores = [
            readiness_factors["rdi_compliance"]["score"],
            readiness_factors["rm_compliance"]["score"],
            min(analysis_result.test_coverage_status.current_coverage, 100.0),
            100.0 if len(blocking_issues) == 0 else 0.0
        ]
        
        overall_readiness_score = sum(factor_scores) / len(factor_scores)
        
        assessment = {
            "overall_readiness_score": overall_readiness_score,
            "phase3_ready": overall_readiness_score >= 80.0 and len(blocking_issues) == 0,
            "readiness_factors": readiness_factors,
            "recommendations": self._generate_readiness_recommendations(readiness_factors),
            "next_steps": self._generate_next_steps(analysis_result)
        }
        
        return assessment
    
    def _format_complete_report(self, report_id: str, analysis_result: ComplianceAnalysisResult,
                              executive_summary: str, detailed_findings: Dict[str, Any],
                              remediation_plan: List[RemediationStep], 
                              phase3_assessment: Dict[str, Any]) -> str:
        """Format the complete compliance report as markdown."""
        
        report_sections = [
            f"# Beast Mode Framework Compliance Report",
            f"**Report ID:** {report_id}",
            f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            executive_summary,
            "",
            "## Detailed Findings",
            "",
            self._format_detailed_findings(detailed_findings),
            "",
            "## Remediation Plan",
            "",
            self._format_remediation_plan(remediation_plan),
            "",
            "## Phase 3 Readiness Assessment",
            "",
            self._format_phase3_assessment(phase3_assessment),
            "",
            "## Appendix",
            "",
            self._format_appendix(analysis_result)
        ]
        
        return "\n".join(report_sections)
    
    def _collect_all_issues(self, analysis_result: ComplianceAnalysisResult) -> List[ComplianceIssue]:
        """Collect all issues from all compliance categories."""
        all_issues = []
        all_issues.extend(analysis_result.rdi_compliance.issues)
        all_issues.extend(analysis_result.rm_compliance.issues)
        all_issues.extend(analysis_result.test_coverage_status.issues)
        all_issues.extend(analysis_result.task_completion_reconciliation.issues)
        all_issues.extend(analysis_result.critical_issues)
        
        # Remove duplicates based on description and affected files
        unique_issues = []
        seen = set()
        for issue in all_issues:
            key = (issue.description, tuple(sorted(issue.affected_files)))
            if key not in seen:
                seen.add(key)
                unique_issues.append(issue)
        
        return unique_issues
    
    def _group_issues_by_type_and_severity(self, issues: List[ComplianceIssue]) -> Dict[ComplianceIssueType, Dict[IssueSeverity, List[ComplianceIssue]]]:
        """Group issues by type and severity for organized remediation."""
        groups = {}
        
        for issue in issues:
            if issue.issue_type not in groups:
                groups[issue.issue_type] = {severity: [] for severity in IssueSeverity}
            groups[issue.issue_type][issue.severity].append(issue)
        
        return groups
    
    def _generate_remediation_description(self, issue_type: ComplianceIssueType, 
                                        severity: IssueSeverity, issues: List[ComplianceIssue]) -> str:
        """Generate description for remediation step."""
        type_descriptions = {
            ComplianceIssueType.RDI_VIOLATION: "Address RDI methodology violations",
            ComplianceIssueType.RM_NON_COMPLIANCE: "Fix RM architectural compliance issues",
            ComplianceIssueType.TEST_FAILURE: "Resolve test failures and coverage issues",
            ComplianceIssueType.DESIGN_MISALIGNMENT: "Align implementation with design specifications",
            ComplianceIssueType.REQUIREMENT_TRACEABILITY: "Establish requirement traceability",
            ComplianceIssueType.ARCHITECTURAL_VIOLATION: "Fix architectural violations"
        }
        
        base_description = type_descriptions.get(issue_type, f"Address {issue_type.value} issues")
        return f"{base_description} ({severity.value} priority) - {len(issues)} issues"
    
    def _estimate_remediation_effort(self, issues: List[ComplianceIssue]) -> str:
        """Estimate effort required for remediation."""
        if not issues:
            return "minimal"
        
        # Use the maximum effort from individual issues
        efforts = [issue.estimated_effort for issue in issues if issue.estimated_effort != "unknown"]
        if not efforts:
            return "medium"
        
        effort_weights = {"minimal": 1, "low": 2, "medium": 3, "high": 4, "critical": 5}
        max_weight = max(effort_weights.get(effort, 3) for effort in efforts)
        
        for effort, weight in effort_weights.items():
            if weight == max_weight:
                return effort
        
        return "medium"
    
    def _extract_affected_components(self, issues: List[ComplianceIssue]) -> List[str]:
        """Extract unique affected components from issues."""
        components = set()
        for issue in issues:
            components.update(issue.affected_files)
        return sorted(list(components))
    
    def _determine_prerequisites(self, issue_type: ComplianceIssueType, severity: IssueSeverity) -> List[str]:
        """Determine prerequisites for remediation."""
        prerequisites = []
        
        if issue_type == ComplianceIssueType.RDI_VIOLATION:
            prerequisites.extend(["Review requirements documentation", "Validate design specifications"])
        elif issue_type == ComplianceIssueType.RM_NON_COMPLIANCE:
            prerequisites.extend(["Review RM interface specifications", "Check architectural guidelines"])
        elif issue_type == ComplianceIssueType.TEST_FAILURE:
            prerequisites.extend(["Analyze test failure logs", "Review test coverage reports"])
        
        if severity in [IssueSeverity.CRITICAL, IssueSeverity.HIGH]:
            prerequisites.append("Coordinate with team lead before implementation")
        
        return prerequisites
    
    def _generate_validation_criteria(self, issue_type: ComplianceIssueType, issues: List[ComplianceIssue]) -> List[str]:
        """Generate validation criteria for remediation steps."""
        criteria = []
        
        if issue_type == ComplianceIssueType.TEST_FAILURE:
            criteria.extend([
                "All failing tests pass",
                "Test coverage meets or exceeds baseline",
                "No new test failures introduced"
            ])
        elif issue_type == ComplianceIssueType.RM_NON_COMPLIANCE:
            criteria.extend([
                "RM interface fully implemented",
                "Module size constraints met",
                "Health monitoring functional"
            ])
        elif issue_type == ComplianceIssueType.RDI_VIOLATION:
            criteria.extend([
                "Requirements traceability established",
                "Design-implementation alignment verified",
                "Documentation updated"
            ])
        
        return criteria
    
    def _analyze_rdi_findings(self, rdi_status) -> Dict[str, Any]:
        """Analyze RDI compliance findings."""
        return {
            "compliance_score": rdi_status.compliance_score,
            "requirements_traced": rdi_status.requirements_traced,
            "design_aligned": rdi_status.design_aligned,
            "implementation_complete": rdi_status.implementation_complete,
            "test_coverage_adequate": rdi_status.test_coverage_adequate,
            "issues_count": len(rdi_status.issues),
            "critical_issues": [i.description for i in rdi_status.issues if i.severity == IssueSeverity.CRITICAL]
        }
    
    def _analyze_rm_findings(self, rm_status) -> Dict[str, Any]:
        """Analyze RM compliance findings."""
        return {
            "compliance_score": rm_status.compliance_score,
            "interface_implemented": rm_status.interface_implemented,
            "size_constraints_met": rm_status.size_constraints_met,
            "health_monitoring_present": rm_status.health_monitoring_present,
            "registry_integrated": rm_status.registry_integrated,
            "issues_count": len(rm_status.issues),
            "critical_issues": [i.description for i in rm_status.issues if i.severity == IssueSeverity.CRITICAL]
        }
    
    def _analyze_test_coverage_findings(self, test_status) -> Dict[str, Any]:
        """Analyze test coverage findings."""
        return {
            "current_coverage": test_status.current_coverage,
            "baseline_coverage": test_status.baseline_coverage,
            "coverage_adequate": test_status.coverage_adequate,
            "failing_tests_count": len(test_status.failing_tests),
            "missing_tests_count": len(test_status.missing_tests),
            "failing_tests": test_status.failing_tests[:10],  # Top 10
            "issues_count": len(test_status.issues)
        }
    
    def _analyze_task_reconciliation_findings(self, task_status) -> Dict[str, Any]:
        """Analyze task reconciliation findings."""
        return {
            "reconciliation_score": task_status.reconciliation_score,
            "claimed_complete_count": len(task_status.claimed_complete_tasks),
            "actually_implemented_count": len(task_status.actually_implemented_tasks),
            "missing_implementations_count": len(task_status.missing_implementations),
            "missing_implementations": task_status.missing_implementations[:10],  # Top 10
            "issues_count": len(task_status.issues)
        }
    
    def _analyze_commit_findings(self, commits) -> Dict[str, Any]:
        """Analyze commit-related findings."""
        total_files_changed = sum(len(c.modified_files) + len(c.added_files) + len(c.deleted_files) for c in commits)
        
        return {
            "commits_count": len(commits),
            "total_files_changed": total_files_changed,
            "recent_commits": [
                {
                    "hash": c.commit_hash[:8],
                    "author": c.author,
                    "message": c.message[:100] + "..." if len(c.message) > 100 else c.message,
                    "files_changed": len(c.modified_files) + len(c.added_files) + len(c.deleted_files)
                }
                for c in commits[:5]  # Most recent 5
            ]
        }
    
    def _generate_next_actions(self, analysis_result: ComplianceAnalysisResult) -> List[str]:
        """Generate next actions based on analysis results."""
        actions = []
        
        if analysis_result.overall_compliance_score < 80.0:
            actions.append("Address critical compliance issues before proceeding")
        
        if not analysis_result.test_coverage_status.coverage_adequate:
            actions.append("Improve test coverage to meet baseline requirements")
        
        if len(analysis_result.test_coverage_status.failing_tests) > 0:
            actions.append("Fix failing tests identified in Phase 2 lessons learned")
        
        if not analysis_result.rdi_compliance.requirements_traced:
            actions.append("Establish complete requirement traceability")
        
        if not analysis_result.rm_compliance.interface_implemented:
            actions.append("Implement missing RM interface methods")
        
        if not actions:
            actions.append("Review and validate Phase 3 readiness assessment")
        
        return actions[:5]  # Top 5 actions
    
    def _generate_readiness_recommendations(self, readiness_factors: Dict[str, Any]) -> List[str]:
        """Generate recommendations for Phase 3 readiness."""
        recommendations = []
        
        if readiness_factors["rdi_compliance"]["status"] == "FAIL":
            recommendations.append("Complete RDI compliance requirements before Phase 3")
        
        if readiness_factors["rm_compliance"]["status"] == "FAIL":
            recommendations.append("Address RM architectural compliance issues")
        
        if readiness_factors["test_coverage"]["status"] == "FAIL":
            recommendations.append("Achieve test coverage baseline before proceeding")
        
        if readiness_factors["blocking_issues"]["status"] == "FAIL":
            recommendations.append("Resolve all blocking issues identified in analysis")
        
        if not recommendations:
            recommendations.append("System appears ready for Phase 3 initiation")
        
        return recommendations
    
    def _generate_next_steps(self, analysis_result: ComplianceAnalysisResult) -> List[str]:
        """Generate specific next steps for Phase 3 preparation."""
        steps = []
        
        if not analysis_result.phase3_ready:
            steps.extend([
                "Execute remediation plan in priority order",
                "Re-run compliance analysis after fixes",
                "Validate all blocking issues are resolved"
            ])
        else:
            steps.extend([
                "Proceed with Phase 3 planning",
                "Schedule Phase 3 kickoff meeting",
                "Begin Phase 3 requirements gathering"
            ])
        
        return steps
    
    def _format_detailed_findings(self, findings: Dict[str, Any]) -> str:
        """Format detailed findings section."""
        sections = []
        
        for category, data in findings.items():
            section_title = category.replace('_', ' ').title()
            sections.append(f"### {section_title}")
            
            if isinstance(data, dict):
                for key, value in data.items():
                    if isinstance(value, (list, tuple)) and len(value) > 0:
                        sections.append(f"- **{key.replace('_', ' ').title()}:** {len(value)} items")
                        if len(value) <= 5:
                            for item in value:
                                sections.append(f"  - {item}")
                        else:
                            for item in value[:3]:
                                sections.append(f"  - {item}")
                            sections.append(f"  - ... and {len(value) - 3} more")
                    else:
                        sections.append(f"- **{key.replace('_', ' ').title()}:** {value}")
            
            sections.append("")
        
        return "\n".join(sections)
    
    def _format_remediation_plan(self, remediation_plan: List[RemediationStep]) -> str:
        """Format remediation plan section."""
        if not remediation_plan:
            return "No remediation steps required."
        
        sections = []
        for i, step in enumerate(remediation_plan, 1):
            sections.extend([
                f"### {step.step_id}: {step.description}",
                f"- **Priority:** {step.priority.value.title()}",
                f"- **Estimated Effort:** {step.estimated_effort}",
                f"- **Affected Components:** {len(step.affected_components)} files"
            ])
            
            if step.prerequisites:
                sections.append("- **Prerequisites:**")
                for prereq in step.prerequisites:
                    sections.append(f"  - {prereq}")
            
            if step.validation_criteria:
                sections.append("- **Validation Criteria:**")
                for criteria in step.validation_criteria:
                    sections.append(f"  - {criteria}")
            
            sections.append("")
        
        return "\n".join(sections)
    
    def _format_phase3_assessment(self, assessment: Dict[str, Any]) -> str:
        """Format Phase 3 readiness assessment section."""
        sections = [
            f"**Overall Readiness Score:** {assessment['overall_readiness_score']:.1f}/100.0",
            f"**Phase 3 Ready:** {'✅ YES' if assessment['phase3_ready'] else '❌ NO'}",
            "",
            "### Readiness Factors"
        ]
        
        for factor, data in assessment["readiness_factors"].items():
            factor_title = factor.replace('_', ' ').title()
            status_emoji = "✅" if data["status"] == "PASS" else "❌"
            sections.append(f"- **{factor_title}:** {status_emoji} {data['status']}")
        
        sections.extend([
            "",
            "### Recommendations"
        ])
        
        for rec in assessment["recommendations"]:
            sections.append(f"- {rec}")
        
        sections.extend([
            "",
            "### Next Steps"
        ])
        
        for step in assessment["next_steps"]:
            sections.append(f"1. {step}")
        
        return "\n".join(sections)
    
    def _format_appendix(self, analysis_result: ComplianceAnalysisResult) -> str:
        """Format appendix with technical details."""
        sections = [
            "### Technical Details",
            f"- Analysis Timestamp: {analysis_result.analysis_timestamp}",
            f"- Commits Analyzed: {len(analysis_result.commits_analyzed)}",
            f"- Overall Compliance Score: {analysis_result.overall_compliance_score:.2f}",
            "",
            "### Issue Summary by Severity"
        ]
        
        all_issues = self._collect_all_issues(analysis_result)
        severity_counts = {}
        for issue in all_issues:
            severity_counts[issue.severity] = severity_counts.get(issue.severity, 0) + 1
        
        for severity in IssueSeverity:
            count = severity_counts.get(severity, 0)
            sections.append(f"- {severity.value.title()}: {count}")
        
        return "\n".join(sections)